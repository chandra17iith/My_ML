# -*- coding: utf-8 -*-
"""LinearRegression_Git.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1Fwr42jY9NUMQNLEKqBEOWUq2uq92KAMT
"""

medical_charges_url = 'https://raw.githubusercontent.com/JovianML/opendatasets/master/data/medical-charges.csv'



from urllib.request import urlretrieve

urlretrieve(medical_charges_url,'medical.csv')

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns

df=pd.read_csv('medical.csv')
df.head()

df.info()

df.describe()

"""#EDA"""

sns.set_style('darkgrid')

"""## Age"""

sns.histplot(df['age'],bins=6)

sns.boxplot(df['age'])

"""##BMI"""

sns.histplot(df['bmi'])

sns.boxplot(df['bmi'])

"""## charges"""

plt.hist(df['charges']);

sns.boxplot(df['charges'])

import plotly.express as px

fig = px.histogram(df,
                   x='charges',
                   marginal='box',
                   color='smoker',
                   color_discrete_sequence=['green', 'grey'],
                   title='Annual Medical Charges')
fig.update_layout(bargap=0.1)
fig.show()

sns.histplot(df['sex'])

"""this shows that there is approximately equal number of male and female who are taking insurance"""

region=df['region'].value_counts().reset_index()
region

plt.pie(region['count'],labels=region['region'],autopct='%.0f%%')

px.histogram(df,x='smoker',color='sex',title='smoker')

fig = px.scatter(df,
                 x='age',
                 y='charges',
                 color='smoker',
                 opacity=0.8,
                 hover_data=['sex'],
                 title='Age vs. Charges')
fig.update_traces(marker_size=5)
fig.show()

fig = px.scatter(df,
                 x='bmi',
                 y='charges',
                 color='smoker',
                 opacity=0.8,
                 hover_data=['sex'],
                 title='Age vs. Charges')
fig.update_traces(marker_size=5)
fig.show()

"""1. we could see that with age the charges are increasing and among somkers it is quite high and we got three cluster
2. in second plot we could see that higher bmi and smokers have higher  charges
"""

px.violin(df,x='children',y='charges')

"""## correlation"""

df['charges'].corr(df['age'])

df['charges'].corr(df['bmi'])

df['charges'].corr(df['children'])

"""to check correlation between categorical data we need to convet it into numerical data"""

# smokers data
smokers=df['smoker'].map({'no':0,'yes':1})
smokers

df['charges'].corr(smokers)

"""we could see that charges correlation with smokers and age is high

Though the correlation is high we cannot say that high charges are caused due to age and smoking because there could be other factors such  as diseases and accidents

# Regression

we will first look for non smokers people relation of age with charges
"""

non_smokers=df[df['smoker']== 'no']
non_smokers

x=non_smokers.iloc[:,:1]
y=non_smokers.iloc[:,-1]

from sklearn.model_selection import train_test_split
x_train,x_test,y_train,y_test=train_test_split(x,y,test_size=0.2,random_state=2)

print(x_train)
print(y_train)

"""while using these model input must be 2d array"""



from sklearn.linear_model import LinearRegression
regressor=LinearRegression()
regressor.fit(x_train,y_train)

regressor.predict([[21]])

plt.scatter(x_test,y_test)
plt.plot(x_test,regressor.predict(x_test),color='orange')

"""model coefficients"""

print(regressor.coef_)

print(regressor.intercept_)



"""checking for smokers and non smokers and the charges"""

# we want to check the realtion between all the dependent variable and our independent variable so we will be considering them as input
# but we have some categorical data so we need to convert it into numerical data
from sklearn.preprocessing import OneHotEncoder
encoder=OneHotEncoder()
enc=encoder.fit(df[['region']])
enc

num_region=encoder.transform(df[['region']]).toarray()
num_region

df[['northeast', 'northwest', 'southeast', 'southwest']]=num_region
df['smoker']=df['smoker'].map({'no':0,'yes':1})
df.head()

"""since there are outliers which can lead to data spillage we will use feature scaling"""

input=['age','bmi','children','smoker']
data=df[input]
data

from sklearn.preprocessing import StandardScaler
sc=StandardScaler()
sc.fit(data)

new_data=sc.transform(data)
new_data

# we will use multiple linear regression
df[input]=new_data
df.head()
final_input=['age','bmi','children','smoker','northeast', 'northwest', 'southeast', 'southwest']
X=df[final_input]
X.head()

X_train,X_test,Y_train,Y_test=train_test_split(X,df['charges'],test_size=0.2,random_state=5)

print(X_train)
print(Y_train)

"""fitting the model"""

regressor.fit(X_train,Y_train)

"""# predicting the result"""

Y_result=np.array(regressor.predict(X_test))
Y_test=np.array(Y_test)
np.set_printoptions(precision=2)
print (np.concatenate((Y_result.reshape(len(Y_result),1),Y_test.reshape(len(Y_test),1)),1))

"""# evaluating the model accuracy

we use r2_score that is to evaluate the model
"""

from sklearn.metrics import r2_score
r2_score(Y_test,Y_result)

"""
the reason for low r2_score could be due to presence of oultiers in our data"""